# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше 30 мегабайт.

У нас уже была программа, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго.

Я решил исправить эту проблему, оптимизировав эту программу. 

Новая цель - программа должна обрабатывать этот файл не более чем за 60 секунд

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я 
решил использовать тесты замеряющие время выполнение программы. Напишем сразу на разрные размеры файлов.  
На момент старта оптимизации имеем следующие значения:
small.json = 12 секунд
medium.json = 95 секунд
large.json = (дофига) секунд


## Гарантия корректности работы оптимизированной программы
Выполнение тестов в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации. 
Поэтому перед началом работы напишем тесты на коррентость работы. 


## Feedback-Loop
1. Запускаем различные профилировщики, выявляем проблемные места
2. Вносим свои правки
3. Тестируем на работоспособность
4. Замеряем производительность

### STEP-0 Начало работы 
1. Развертываем систему
2. Пишем тесты 
3. Настраиваем профилировщики
4. Замеряем производительность (medium:95с, large:много)

### STEP-1 Начало работы 
1. Смотрим в pghero - на что уходит больше всего времени; Видим что очень много запросов идет к справочним данным(автобусы, города и т.д.).
2. Так как справочных данных точно меньше рейсов - не будем искать эти значения на каждой итарации.
3. Замеряем производительность (medium:36с, large:284c). Ускорение почти 3 раза, хороший результат. Идем дальше 

## Результаты

## Защита от регрессии производительности
