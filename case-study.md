# Case-study оптимизации

## Актуальная проблема

Необходимо импортировать файл с данными fixtures/large.json с 100_000 трипов менее чем за 1 минуту.

У нас уже была кфлу задача, которая умела делать нужную обработку.

Но она недостаточно производительна
Так:
 small.json с 1K трипов обрабатывается за 10 секунд
 medium.json с 10K трипов обрабатывается за 80 секунд

По грубым оценкам large.json с 100K трипов будет обрабатываться не менее 800 секунд.

## Формирование метрики
Для анализа влияния изменений на скорость работы возьмем время обработки файла small.json - 10 секунд  

## Гарантия корректности работы оптимизированной программы
Для проверки корректности работы обновленной программы был написан тест, который заполнял БД данными из файла example.json, потом выгружал БД в json и сравнивал с исходным.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 10-20 секунд

1. Для получения обратной связи я пользуюсь командой которая проверяет корректность работы, потом замеряет метрику
1. После чего я запускаю профайлер(ы) и по отчетам анализирую изменения
 
## Вникаем в детали системы, чтобы найти главные точки роста

Для того, чтобы найти "точки роста" для оптимизации я воспользовался
- гемом memory_profiler

Вот какие проблемы удалось найти и решить

### Находка №1
- memory_profiler показал, что самую большую часть памяти среди объектов занимают массивы 
- решено отказаться от сбора данных в массивы, а обрабатывать и записывать отчет по каждому пользователю
- метрика уменьшилась до 15 мегабайт
- как изменился отчёт профилировщика теперь массивы занимают 13.36 MB

### Изменение метрики
 Замеряю использованную память при обработке 200_000 строк - 15 MB
 Замеряю использованную память при обработке 1_500_000 строк - 17 MB
 Замеряю использованную память при обработке всего рабочего файла - 17 MB
 
## Результаты
В результате проделанной оптимизации удалось обработать файл с данными.
Грубое измерение метрики показывает, что программа укладывается в бюджет.

Удалось улучшить метрику системы и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности

Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был написан тест с фиксацией размера потребляемой памяти при обработки файла с 500 тысячами строк. 

## Checklist
- [+] Построить и проанализировать отчёт гемом `memory_profiler`
- [+] Построить и проанализировать отчёт `ruby-prof` в режиме `Flat`;
**не нагляден**

- [+] Построить и проанализировать отчёт `ruby-prof` в режиме `Graph`;
**удобен, но надо разносить код в именованые методы**

- [+] Построить и проанализировать отчёт `ruby-prof` в режиме `CallStack`;
**очень удобное представление, но надо разносить код в именованые методы**

- [+] Построить и проанализировать отчёт `ruby-prof` в режиме `CallTree` c визуализацией в `QCachegrind`;
**В убунту не нашел QCachegrind, есть kcachegrind, там нет отображения в виде блок-схемы**

- [+] Построить и проанализировать текстовый отчёт `stackprof` для рабочего файла

 При попытке получить данные по методу "Oh My ZSH!" воспринимает опцию --method Object#work как какую-то команду для себя. Надо писать --method "Object#work" 
 
 **Не наглядно**
 - [-] Построить и проанализировать отчёт `flamegraph` с помощью `stackprof` и визуализировать его в `speedscope.app`;
**Не понял как**

- [+] Построить график потребления памяти в `valgrind massif visualier` и включить скриншот в описание вашего `PR`;
![](valgrind-massif-visualizer.png)

# Что еще можно сделать

### Находка №2
- По опыту известно, что библиотека json исолпьзует довольно много памяти
- форматировать вывод самостоятельно
- для ускорения feedback-loop взята метрика: память для обработки 1_500_000 записей, она составляет 17700 - 18000 килобайт
- Метрика значимо не изменилась

### Находка №3
- отчет MemoryProfiler показывает, что очень много раз выделяются одинаковые строки
- использовать константы
- Метрика значимо не изменилась
- использование памяти с выключенным GC снизилось с 98 до 90 мб.

### Находка №4
- отчет MemoryProfiler показывает, что все еще много раз выделюятся одинаковые строки при определение типа записи в исходном файле и разбиении их
- оптимизировать эти строки
- Метрика снизилась до 16900-17100 кб
- использование памяти с выключенным GC снизилось с 90 до 86 мб. 
- ** читаемость кода снизилась**

### Находка №5
- отчет MemoryProfiler показывает, что строка "result.json" выделяется столько раз, сколько происходит записаь в файл, несмотря на использование константы 
- один раз открыть файл для записи
- Метрика значимо не изменилась
- использование памяти с выключенным GC снизилось с до 78 мб.

Отчет MemoryProfiler не показывает избыточного выделения памяти.  

 

