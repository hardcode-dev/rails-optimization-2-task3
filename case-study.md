# Case-study

## Актуальная проблема
В проекте используется импорт расписаний из `json` файла.
Импорт отрабатывал в пределах минуты, пока файл имел до 10k импортируемых записей. С увеличением объема данных импорт данных стал выполняться слишком долго.
При первом рассмотрении, стало понятно что загрузка данных из файла производится неэффективно.
Кроме того, с увеличением количества записей, страница расписаний автобусов загружается слишком медленно.

Мною принято решение:
 - исправить и оптимизировать код reke-задачи, которая отвечает за импорт
 - найти и устранить проблемы, замедляющие формирование страниц расписаний

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я решил использовать такую метрику: *время обработки файла с 1000 тыс. записей*

## Гарантия корректности работы оптимизированной программы
Наше Rails приложение не имело тестов. Для гарантии корректности внесенных изменений я добавил 2 теста, первый отвечает за корректность импорта, второй отвечает за корректность вывода данных на странице рассписаний автобусов. Выполнение  теста на корректность импорта в фидбэк лупе позволяет не допустить изменения логики работы импорта, а тест на контроллер позволяет не допустить изменения в отображении контента при оптимизации контроллера.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: *как вы построили feedback_loop*

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я разнес код по отдельным методам `flush_db`, `fetch_from`, `fetch_to`, `fetch_services`, `fetch_bus`, `update_bus` - это помогло добавить информативности отчетам профилировщиков. Затем я воспользовался инструментами:
  - *benchmark realtime* показал, что файл с `small.json` скрипт обрабатывает ~6 сек, а 10,000 ~60 сек. Можно предположить, что асимптотика линейная
  - *stackprof wall & speedscope.app* показал, что половину всего времени (~3 сек на файле с `small.json`) занимает работа с базой во главе с PostgreSQLAdapter
  - *ruby_prof graph* показал 6788 обращений к методу #transaction
  - *ruby_prof walltime callstack* показал наибольшее время выполнения методов #update_bus и #fetch_services
  - *memory_profiler*
  - *pghero* после активации расширения и загрузки `medium.json` показал самый долгий запрос `SELECT "services".* FROM "services" INNER JOIN "buses_services" ON "services"."id" = "buses_services"."service_id" WHERE "buses_services"."bus_id" = $1`
  - *pg_query*
  - *strong_migrations*
  - *activerecord-import*
  - *rack-mini-profiler*

## Импорт

###  Находка №1
- большое количество транзакций
- добавил кэширование Service и City, убрал `Bus#update`, задействовал гем `activerecord-import`: сделал импорт автобусов и поездок, а так-же обновление join-таблицы `buses_services` в батчах по 1000
- количество транзакций снизилось до c 6788 до 24, время обработки файла `small.json` снизилось с ~6 до до ~1.9 сек
- проблема транзакций перестала быть главной точкой роста

###  Находка №2
- большое количество обращений к методу fetch_bus `Bus.find_by(name, number)`, `pghero` посоветовал создать индекс на bus_id в таблице `buses_services`
- я добавил составной индекс `add_index :buses, [:number, :model], algorithm: :concurrently` но потом решил кэшировать все автобусы, тем самым избавился от лишнего запроса к базе.
- добавил индекс `add_index :buses_services, :bus_id, algorithm: :concurrently`
- количество запросов `SELECT  "buses".* FROM "buses" WHERE "buses"."number" = $1 AND "buses"."model" = $2 LIMIT $3` свелось к нулю
- проблема транзакций перестала быть главной точкой роста время обработки файла `small.json` снизилось с ~2 до до ~1.4 сек, `medium.json` до 7 сек., `large.json` до ~80 сек.

###  Находка №3
- stackprof wall и ruby_prof walltime callstack указывают на то, что работа `activerecord-import` занимает основное время и все старания по оптимизации ни к чему не приводят.
  - 1312  (  100.0%)  JSONImporter#call
  -  726  (   55.3%)  JSONImporter#import_trips
  -  403  (   30.7%)  JSONImporter#import_buses
  -   56  (    4.3%)  JSONImporter#build_bus
- начал разбираться и понял, что что-то не так с `activerecord-import`, попутно наткнулся на issue https://github.com/zdennis/activerecord-import/issues/615. В итоге нашел причину: чтобы не загружать данный гем при старте Rails в Gemfile при добавлении я проставил require: false, но при этом в сервисе надо было сделать require дополнительным модулям.
```
require 'activerecord-import' # было
require 'activerecord-import/base' # добавил
require 'activerecord-import/active_record/adapters/postgresql_adapter' # добавил
```
- проблема долгого импорта перестала быть главной точкой роста время обработки файла `small.json` снизилось с ~1.4 до до ~0.2 сек, `medium.json` до 1.4 сек., `large.json` до ~12 сек.


## Рендеринг
###  Находка №1


## Завершение
- добавил спек для фиксации метрики `rspec-benchmark` с пороговым значением 0.2 мс для обработки `small.json`
- посредствам `memory_profiler` проверил, что для обработки `large.json` требуется ~234.91 мб памяти из них 167.26 мб аллоцируется при чтении файла целиком.


## Результаты


## Защита от регрессии производительности