# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было импортировать большой файл с данными

У нас уже была программа на `ruby`, которая умела это делать, но на большом обьеме (32Мб) дождаться ее завершения не удалось

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: 
время импорта маленького файла json 

time bin/rake reload_json[fixtures/small.json]
real	0m10.243s
user	0m7.906s
sys	0m0.694s


## Гарантия корректности работы оптимизированной программы

Для того чтобы ничего не сломать я вынес логику импорта из рейк таска в класс IMporter
И добавил тест который загружает example.json и сверяет ответ сервера на запрос Москва-Самара с эталоном (до оптимизаций)


## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: 
В качестве фидбек лупа я использовал команду импорта маленького файла
bin/rake reload_json[fixtures/small.json] что позволило тратить 10с на фидбек луп

## Вникаем в детали системы, чтобы найти главные точки роста


### Ваша находка №1

## Результаты


## Защита от регрессии производительности

