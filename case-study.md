# Case-study оптимизации

## Актуальная проблема

В нашем проекте возникла проблема. Необходимо было импортрировать данные из большого JSON-файла, затем отобразить данные в веб-интерфейсе. Программа успешно работала на файлах небольшого размера, но с увеличением объема исходных данных их обработка занимала слишком много времени.

Я решил исправить эту проблему:
- оптимизировать импорт данных;
- оптимизировать формирование и вывод итоговых данных.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я решил измерять время обработки файла `medium.json` с 10.000 записей.

## Гарантия корректности работы оптимизированной программы
Для гарантии корректной работы программы в процессе оптимизации я реализовал перфоманс-тест импорта для загрузки данных из файла `example.json` менее чем за 10 секунд. Также я добавил feature-тест, чтобы отслеживать отсутствие изменений в выводе итоговых данных.

## Feedback-Loop
Для того, чтобы иметь возможность проверять гипотезы, я выстроил feedback-loop, который позволил мне получать обратную связь по эффективности сделанных изменений менее чем за минуту.

Я замерил время выполнения rake-задачи импорта на разных объемах данных с помощью `Benchmark.realtime`:
```
rake "reload_json[fixtures/small.json]"
Finished in 6.93s
```
```
rake "reload_json[fixtures/medium.json]"
Finished in 50.83s
```
```
rake "reload_json[fixtures/large.json]"
Finished in Finished in 463.21s
```

## Оптимизация импорта данных

Прежде всего я решил вынести все методы в отдельный класс для удобства работы и тестирования, в `Rake` оставил только обертку.

Изучение логов показало, что в процессе импорта происходит много лишних SQL-запросов к базе. Для того чтобы устранить эту проблему я решил сразу импортировать в базу данные по `City`, `Bus`, `Service` и сохранить их в памяти в виде хэшей, чтобы работать с ними, избегая лишних обращений к базе.

Начиная с `Rails 6` для массового импорта можно использовать метод `insert_all`, я решил немного усложнить задачу (частый случай из жизни: legacy-код, для которого нежелательно обновление гемов ввиду вероятности поломки зависимостей) и сохранить существующие версии, а для импорта использовать гем `activerecord-import`. Результаты порадовали, программа стала работать быстрее более чем в 20 раз:
```
rake "reload_json[fixtures/medium.json]"
Finished in 2.42s
```
Также я использовал гем `oj` для работы с JSON, итоговые результаты:
```
rake "reload_json[fixtures/small.json]"
Finished in 0.42s
```
```
rake "reload_json[fixtures/medium.json]"
Finished in 2.31s
```
```
rake "reload_json[fixtures/large.json]"
Finished in 23.29s
```
Таким образом, мне удалось обработать файл `large.json` примерно за 24 секунды, что в два с половиной раза меньше обозначенного бюджета.

## Оптимизация вывода итоговых данных
Изначальная скорость загрузки страницы с данными из файла `large.json` составила `12041.0 ms`. Следуя рекомендациям гема `bullet`, я добавил предзагруку `Trip.preload([bus: :services])`, таким образом мне удалось уменьшить время загрузки до `7665.7 ms`.

Далее я начал изучать отчеты `rack-mini-profiler` и `pg_hero` на предмет повторяющихся запросов и других точек роста, после чего добавил индексы на самые популярные запросы — время рендеринга уменьшилось незначительно, но о базе тоже надо заботиться. Очевидно, что главные точки роста находятся в рендеринге. Я пытался воспользовался `RailsPanel`, чтобы проанализировать ситуацию, но расширение часто зависало, хотя мне и удалось кое-что выяснить.  

Пересилив сиюминутное желание переместить все паршелы (`delimiter`, `service`, `services` и `trip`) непосредственно в `index`, я решил оставить паршел `trip`, отрендеривая его с применением коллекции. После рефакторинга вьюх я добился ускорения рендеринга еще в три раза: `2327.2 ms`.

И вот настал тот момент, когда после долгих попыток выжать хоть что-то существенное я понял, что забывал отключать гем `bullet` когда измерял метрику. Итоговое время рендеринга страницы с 1000+ записей составило `282.7 ms`, на этом я решил остановиться.

## Результаты
В результате оптимизации удалось добиться следующих результатов:
- оптимизация импорта данных: время обработки файла `large.json` уменьшилось в 16 раз с `463.21s` до `23.29s`;
- оптимизация рендеринга итоговых результатов: время рендеринга страницы с 1000+ записей уменьшилось до `282.7 ms`.
