# Case-study оптимизации

## Заметки от себя
1. При импорте файла 1M.json и 10M.json возникли некоторые небольшие проблемы.
   
   - Присутствовали города с проблемами, когда в моделе City стоит проверка на то, чтобы города были без пробелов
   - Были повторяющиеся number у Bus. В моделе Bus имеется проверка на уникальность number.
   
   Данные моменты я решил исправить так.
   При импорте City (в service ParseStreamJsonService) я убираю у всех городов пробелы, используя метод tr!
   
   А с автобусами я создал уникальный индекс по двум полям, number и model. А также поправил модель Bus, и указал, что уникальность number будет в пределах model.
   То есть уникальные два значения.
   
   По автобусам - можно было еще проводить проверку, что если есть такой number, то пропускать импорт такой записи. А по городам можно было убрать проверку на наличие пробелов.
   
   Если мои способы(добавление уникальности по двум полям и избавление пробелов у всех городов) решения этих моментов не являются корректными, то я изменю это, например, на вариант с разрешением городов с пробелами и проверка number автобусов перед их созданием (либо ловить exception уникальности)
2. Содержимое rake таски я перевел в service. Это позволяет удобнее тестировать, а также возможность использовать в нескольких места, если такое потребуется. Плюс так удобнее запускать из консоли и дебажить  

3. Было создано два сервиса - Один работает в обычном варианте (берет и грузить все в память), другой в потоковом. 
4. В сервисе ParseStreamJsonService идет проверка на тип файла и если это Gzip, то идет непосредственный парсинг с gzip формата(возможность гема Yaji-ruby). То есть необходимости распаковывать нет, но такой файл будет обрабатывать слегка дольше (нужно ему время на открытие файла)
5. Также, в обоих сервисах идет "складирование" городов и автобусов, что позволило увеличить производительность парсинга. Но все это храниться в хеше и такое, по идее, желательно хранить в Redis, и оттуда доставать. Но думаю в учебном проекте можно использовать хеш, хотя я понимаю, что это не самый лучший способ
6. В первом сервисе ParseJsonService я намеренно хотел оптимизировать работу с large.json без использования построчного чтения файла
7. Rake таска `reload_json` для запуска сервиса, который читает файл полностью. Rake таска `reload_stream_json` для запуска потокового чтения файла
8. Вызывать в консоли сервисы можно так
   ```
   ImportTrips::ParseJsonService.call(file_path: path_to_file)
   ImportTrups::ParseStreamJsonService.new(file_path: path_to_very_large_file).call
   ```
9. Согласно рекомендациям по стилу rails ([rails style guide](https://github.com/arbox/rails-style-guide/blob/master/README-ruRU.md#has-many-through)) рекомендуют использовать `has_many :through`, но в данном учебном проекте я не стал это делать, хотя исключительно `has_many :through` использую в рабочих проектах
10. Если посмотреть во view - то достаточно много partial`s, которые, вроде как, необязательно должны быть, так как не так много там html + неиспользуется в других местах. От partial's я тоже не стал избавляться. Надеюсь, не засчитают это минусом)

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными формата JSON.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:

Для начала я решил пропарсить файл medium.json (10к трипов)
Я обернул rake таску в Benchmark.measure - в итоге, время обработки(парсинг) файла medium.json составило 84 секунды

А работы должны проводиться с файлом large.json, который имеет в 10 раз больше данных (100к трипов)

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений

Вот как я построил `feedback_loop`:

- Профилирование
- Обнаружение точки роста(PGHERO)
- Оптимизация
- Запуск тестов, чтобы убедиться, что программа возвращает нужный результат(работает верно)
- Метрика (использование Benchmark.measure, чтобы определить время выполнения)

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался PGHERO.
Также, я использовал ruby-prof. Особой пользы в данном случае не было, но было интересно посмотреть, какие вызываются классы/методы фреймворка Ruby on Rails и сколько они потребляют

Вот какие проблемы удалось найти и решить

### Ваша находка №1
Воспользовался PGHERO и он мне выдал следующую картину (после запуска скрипта импорта medium.json)
![pghero_mediun first](https://i.ibb.co/pn0HXGv/first-hw3.jpg)

Тут мы видим очень большое кол-во вызовов разных запросов. А также рекомендации по созданию индексов

Решил попробовать оптимизировать запрос к городам, исключительно в качестве разминки =)

Так как городов не так много, как самих запросов, я решил, «закешировать» города. А именно создать хеш cities, в него добавлять записи  при обращение к бд, если таких записей нет.
Сделав это, видим следующий результат

Время работы скрипта 67 секунд
10к запросов городов улетели из списка queries в PGHERO
![pghero_medium_second](https://i.ibb.co/vm3Qvhx/second-hw3.jpg)

Итак, с городами это мы провели разминку =) 

### Ваша находка №2
Главная точка роста это первый запрос, который идет к services и занимает больше всего времени.
Но просмотрев его, сходу я не придумал, как его оптимизировать.

Поэтому я решил оптимизировать остальные 3 запроса по списку. А именно добавить рекомендуемые индексы, а также избавиться от избыточных запросов к сервисам (мы знаем, что их может быть только 10 штук, так как эта логика заложена в модель Service)

После этих правок, время выполнение скрипта составило 42 секунды

А результат PGHERO таков

![pghero_medium_third](https://i.ibb.co/0FKyB8R/third-hw3.jpg)

Видим, что мы смогли избавиться от избыточных запросов к services, а также нет предупреждений о том, что необходимо поставить индекс.

### Ваша находка №3
Далее, я занялся главной точкой роста (Выборка сервисов с INNER JOIN buses_service )
а также использовал гем activerecord-import для того, чтобы сразу пачками импортировать
В итоге, время выполнения 15 секунд и PGHERO показывает следующее

![pghero_medium_fourth](https://i.ibb.co/56HS4M5/fourth-hw3.jpg)

При запуске на файле large.json — время выполнения 58 секунд при отключенной валидации при импорте через activerecord-import и 72 секунды при включенной.
Так как нам важны валидные данные, отключать проверку в этом случае, думаю, не стоит.
Поэтому продолжаем оптимизировать

### Ваша находка №4
Мы видим (по предыдущему результатам PGHERO), что идет выборка автобусов 19 000 раз, при этом вставка уникальных автобусов 1000 штук. 

Думаю, что мы можем позволить себе засунуть это в хеш и, вместо запросов, дергать данные оттуда. В учебном проекте, думаю, такой вариант жизнеспособен, в ином случае такое, как мне кажется, лучше держать в Redis

Использовав такой подход, производительность заметно выросла

Время выполнение по бенчмарку 21 секунды
```
20.502660   0.217497  20.720157 ( 21.437042)
```

А PGHERO говорит о следующем
![visualize second](https://i.ibb.co/0YwBfDt/fifth-hw3.jpg)

В бюджет мы уложились, тесты проходят, поэтому я смело могу приступить к выполнению бонусной части домашнего задания

### Ваша находка №5 (бонусная часть)
Бонусная задача заключается в том, что необходимо пропарсить файлы гораздо большим объемом. Самый крупный файл весит, в развернутом состоянии, примерно 3GB
То есть, надо парсить JSON  в потоковом состоянии, а также грузить все в БД тоже в потоковом режиме.

Подходящим, на мой взгляд, оказалась библиотека (gem) — Yaji-ruby.
Этот гем позволять также парсить json в упакованном состоянии, что для нас это тоже будет полезно 

Реализовать чтение и запись в потоковом состоянии я решил уже в другом классе (ParseStreamJsonService).

В итоге, запуск такого режима импорта на файле large.json показал следующий результат
По времени чуть больше 8 секунд
```
5.717256   0.259561   5.976817 (  8.223414)
```

Существенно быстрее, чем предыдущий сервис (ParseJsonService), который грузил файл полностью в память.

А PGHero показал следующее

![pghero_bonus](https://i.ibb.co/bsfBtf3/bonus-hw3.jpg)

А работа с файлом 1M.json заняла чуть больше минуты
```
49.756127   2.840960  52.597087 ( 61.012836)
```

### Ваша находка №6 (controller, view)
Приступаем к отображению рейсов
Ставим rack-mini-profiler и bullet

Открываем страницу /автобусы/Самара/Москва — и сразу смотрим в PGHERO
Он показывает следующую картину

![phhero_view_1](https://i.ibb.co/R7M9NTn/view-1.jpg)

Видим рекомендации по созданию индексов, а также то, что выборка сервисов с автобусами являются главной точкой ростой и вызываются на этой странице 116 раз(на самом деле было 58 раз, просто забыл почистить PGHERO статистику).

Также, на страница bullet нам подсказывает, куда добавить includes
![bullet_view_1](https://i.ibb.co/nzVW63t/view-bullet-1.jpg)

Послушаем эти советы, только вместо includes, воспользуемся preload

И теперь результаты таковы
![pghero_view_2](https://i.ibb.co/FH0yWrc/pghero-view-2.jpg)

Мы видим, что ситуация стала значительно лучше, но проблема все еще есть

### Ваша находка №7 (controller, view)
На предыдущем скриншоте результатов PGHERO мы видим, что на каждый рейс запрашивались дополнительно сервисы. 

А bullet начал говорить о том, что у нас все немножко плохо
![bullet_view_2](https://i.ibb.co/qxQs2d3/bullet-view-2.jpg)

Попробуем добавить preload preload(bus: :services)

После этого ситуация улучшилась
![pghero_view_3](https://i.ibb.co/G9dYstj/pghero-view-3.jpg)

Но теперь нам нужен индекс

### Ваша находка №8 (controller, view)
На предыдущем скриншоте результатов PGHERO  увидели, что есть нехватка индекса.

После добавления индекса, PGHERO показал следующий результат

![pghero_view_final](https://i.ibb.co/d5JkQsP/pghero-view-final.jpg)

То есть, нет запросов, которые прям значительно ухудшали ситуацию, сейчас выполняется все вполне приемлемо.

Также, был заменен `@trips.count` на `@trips.size` в представление, дабы избежать дополнительного запроса

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось обрабатывать файл large.json за 21 секунду при обычном парсинге и за 8 секунд при потоковом чтении и записи

Также, удалось оптимизировать отображение самих рейсов в представлении

А также научился пользоваться PGHero, ощутил его мощь и пользу.
А еще узнал про потоковую запись и чтение в postresql и даже есть где это использовать в реальном проекте =)

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы - я написал тест, который проверяет время выполнение сервисов
