# Оптимизация взаимодействия с БД

## User story

Тут должны быть описаны пользовательские истории которые определят целевые 
значения метрик, но так как их нет, то я их придумаю.

### Импорт данных

Для проведения нагрузочного тестирования на тестовом стенде необходимо регулярно
импортировать различные наборы данных о расписании автобусов из больших
текстовых файлов, файлы получаются из внешних систем поэтому нет возможности
осуществлять накат предварительно сформированного sql-дампа.
Текущее решение не позволяет выполять эту операцию быстро из-за чего подготовка
тестовой среды происходит непозволительно долгое время чем блокирует всю
дальнейшую работу пайплайна.

### Отображение расписания

Система веб-аналитики показывает, что при просмотре расписаний на популярных
направлениях пользователи не дожидаются загрузки страницы и уходят с сайта.
Британские учёные доказали, что финансово успешные web-проекты должны
формировать основной документ страницы не больше чем за 300 миллисекунд.

## Что сделать

* Нужно оптимизировать механизм перезагрузки расписания из файла 
  * файл large.json должен обрабатывался не более минуты.
* Необходимо ускорить отображение расписаний
  * страница автобусы/Самара/Москва должна открываться менее чем за 300мс

# Оптимизация импорта

## Основные инструменты для исследования

* perfolab (чуть доведенный до ума фреймворк разработанный мной в рамках предыдущего задания)
* stackprof
* speedscope

## Основные проблемы на начальном этапе

* Отсутствие тестов.
  * Нет возможности проводить оптимизацию без опасения что-либо сломать.
* Вся процедура выполняется в едином блоке кода без разбивки на методы.
  * Сложно выяснить что именно вызывает основные проблемы.

## Первичный анализ

Замеры времени импорта показывют что оно растёт приблизительно линейно в
зависимости от размера файла, что несложно увидеть и при обычном чтении
оригинального скрипта. Инструменты показывают что большую часть времени
выполняются запросы на поиск и создание объектов.

Уже на данном этапе очевидно что необходимо кардинально уменьшать количество
отправляемых в БД запросов. Внимательное изучение скрипта показывает что
от запросов на поиск данных в бд можно полностью избавиться, так как изначально
в БД нет никаких данных. Уменьшения же количества запросов на вставку данных
можно добиться с помощью массового создания объектов.

Для удобства можно воспользоваться библиотекой activerecord-import.

## Первые шаги

* Код импорта был выделен в отдельный класс для удобства анализа и тестрования.
* Был написан минимальный тест на корректнось работы скрипта.
* Произведен первичный анализ скрипта на файле small.json

|            benchmark           | Previous | Current |      Diff %     |
|--------------------------------|----------|---------|-----------------|
| total                          |          | 3014ms  |                 |

## Выделение методов

Хоть у нас и есть инсайдерская информация о частоте появления уникальных записей
различных моделей, всё же будет намного удобнее ориентироваться в результатах
профилировщиков если выделить создание разных объектов в отельные методы.
Как и ожидалось, выделение методов не оказало значительного влияния на время
работы импорта.

|            benchmark           | Previous  | Current |      Diff %    |
|--------------------------------|-----------|---------|----------------|
| total                          | 3014ms    | 2976ms  | -1%            |

Кроме того, сразу видно что 59% времени уходит на создание и обновление записей
об автобусах.

## Оптимизация создания автобусов

При внимательном изучении механизма создания автобусов становится очевидным
что выполняются множество лишних действий:
* лишнее обновление записи, имя модели выставляется уже после создания;
* обновление модели и услуг происходит каждый раз, даже если автобус уже есть в БД;
* поиск автобусов в БД, хотя их создание происходило в этом же скрипте.

Результат исправления:

|            benchmark           | Previous | Current |      Diff %     |
|--------------------------------|----------|---------|-----------------|
| total                          | 2976ms   | 2628ms  | -11%            |

## Оптимизация выставления связей между автобусами и услугами

Результат улучшения оказался не столь существенным для файла small.json,
на обновление данных об услугах в автобусах всё ещё уходит около 45% времени,
это говорит о том, что нужно оптимизировать выставление связей между автобусами
и сервисами.
Оптимизировать это можно несколькими способами, но самым эффективным будет
массовое выставление связей уже после создания записей автобусов.

Результат исправления:

|            benchmark           | Previous | Current  |      Diff %     |
|--------------------------------|----------|----------|-----------------|
| total                          | 2628ms   | 1450ms   | -44%            |

## Создание сервисов

На поиск и создание сервисов уходит 34% времени. При этом заранее известно что
их ровно 10 (валидация на имя в модели и подсказка в Readme.md).
Вместо того чтобы осуществлять поиск и создание, можно было бы заранее создать
все 10 записей игнорируя данные из json. Но так как у нас есть требование на
полное отсутствие функциональных изменений, то отказываемся от этой идеи, так
как для файла example.json должно быть создано только 3 сервиса.
Так или иначе можно избавиться от необходимости поиска записей в БД.

|            benchmark           | Previous | Current  |      Diff %     |
|--------------------------------|----------|----------|-----------------|
| total                          | 1450ms   | 885ms    | -38%            |

## Импорт расписаний

Наконец дошли до основной модели. Замеры показывают что 39% времени уходит на
создание записей модели Trip.
Так же как и с привязкой услуг к автобусам используем массовый импорт заранее
подготовленных данных.

|            benchmark           | Previous | Current  |      Diff %     |
|--------------------------------|----------|----------|-----------------|
| total                          | 885ms    | 626ms    | -29%            |

## Оптимизация создания городов

Теперь 38% времени уходит на создание записей модели City. Так же как и ранее
убираем поиск сохраненных записей в БД.

|            benchmark           | Previous | Current  |      Diff %     |
|--------------------------------|----------|----------|-----------------|
| total                          | 626ms    | 357ms    | -43%            |

## Предварительные итоги

Проверка иморта файла large.json показала что время импорта составляет около
12 секунд, что уже существенно ниже исходных требований.
На файле small.json stackprof показывает что около 50% времени уходит на
создание записей модели автобуса, однако на файлах medium.json и large.json
на данное действие уходит намного меньше времени в процентном отношении (24%
и 3,5% соответственно).

На больших файла на первый план выходит уже работа массового импорта записей.
Причём много времени уходит на валидации записей.
Можно было бы отключить их, но строго говоря это действие является
функциональным изменением, так как оригинальный скрипт просто бы пропустил
навалидные записи.

Дальнейший анализ будет производиться на файле medium.json, но уже после
выполнения второй части задания. Вероятно решение второй части зададания 
несколько замедлит работу импорта, так как наверняка в рамках изменений 
потребуется добавить индексы, что должно несколько замедлить добавление записей.

# Ускорение отображения расписаний

## Основные инструменты для исследования

* rails log
* rack-mini-profiler

## Первичный анализ

Страница расписания рейсов из Самары в Москву генерируется около 2 секунд
в dev-окружении и около 1,2 секунды в production окружении.
При этом время на работу с БД относительно небольшое:
Production: Views: 934.6ms | ActiveRecord: 356.4ms
Development: Views: 1714.1ms | ActiveRecord: 242.4ms.
Вызывает вопросы почему время на ActiveRecord в Development-окружении
отображается как меньшее, пока решил отложить этот вопрос.
Очевидно тут стоило бы заняться в первую очередь вопросом генерации вьюх,
но так как тема задания касается оптимизации БД, то пока проигнорируем это.

## Анализ с использованием rack-mini-profiler

Для удобства решил выполнять основную работу по оптимизации в development.
Подключение rack-mini-profiler сразу же ухудшило показатели
Views: 5273.9ms | ActiveRecord: 737.0ms, тем не менее общая картина должна
быть той же.

## Двойная проблема N+1 (автобусы и сервисы)

Даже без установки bullet в логе видно огромное количество однотипных запросов.
Добавил preload на обе модели.
Время генерации страницы сократилось вдвое, при этом на работу ActiveRecord
теперь уходит всего 22ms (если верить логу rails)
Views: 2928.7ms | ActiveRecord: 22.0ms.
Предполагаю что уменьшение времени на вьюхи обусловлено меньшим временем для
формирования отчёта rack-mini-profiler, но это не точно, так как в production
время тоже значительно уменьшилось.
С отключенным rack-mini-profiler:
Production: Views: 345.8ms | ActiveRecord: 22.2ms
Development: Views: 984.0ms | ActiveRecord: 17.3ms

## Оптимизация рендеринга

Как бы ни хотелось продолжить, но в дальнейшей оптимизации запросов не вижу
смысла, можно было бы еще добавить индекс на start_time таблицы trips,
но это ускорило бы и без того быстрый запрос (на моём ноутбуке этот запрос
выполняется за 3мс) и в любом случае это не позволит добиться придуманного мной
целевого показателя.

Поэтому единственным способом достижения цели является оптимизация реднеринга.
Используя механизм render collection удалось добиться формирования страницы
в production окружении в пределах 150-200 мс без использования кэширования,
что соответствует целевым показателям.

# Оптимизация импорта: империя наносит ответный удар

Снова придумываю пользовательскую историю.
Отдел продаж начал продавать функционалость позволяющую провести миграцию
с системы конкурентов.

Объем данных в исходной системе может быть произвольным.
Формат данных для импорта тот же и не может быть изменён.

## Что сделать

Нужно оптимизировать механизм загрузки данных из файла таким образом, чтобы
потребление памяти не зависило от объёма данных.

## Общий план действий

Необходимо реализизовать потоковое чтение и импорт данных.
Основными проблемами является то, что данные нужно импортировать в несколько
таблиц, нет возможности продолжительное время "накапливать" данные для
последующего импорта.
До начала работ по оптимизации желательно уточнить фунциональные требования:
* необходимось сохранения вызова валидаций при создании объектов;
* необходимость выполнения всех действий в рамках единой транзакции.

Для интереса я буду исходить из необходимости сохранения старого поведения,
поэтому вариант с импортом данных напрямую в PG из Readme.md будет
нецелесообразен, так как всё равно портребует создания объектов для запуска
валидаций.

Текущая идея состоит в том, чтобы при потоковом чтении из исходного файла
формировать буфер записей определённого размера с последующим импортом
с помощью activerecord-import.

## Потоковое чтение и импорт блоками

Для реализации потокового чтения была использована библиотека oj.
Благодаря использованию метода `Oj.sc_parse` и написанию простого
вспомогательного класса удалось сохранить код практически в неизменном виде.
Кроме того была использована встроенная библиотека zlib для возможности
поточного чтения из gz-файла, тем самым избавляя от необходимости распаковки
файлов большого размера.

Оверхед добавленный механизмом потокового чтения оказался незначительным.

|            benchmark           | Previous  | Current |      Diff %     |
|--------------------------------|-----------|---------|-----------------|
| total                          | 1498ms    | 1531ms  | 2%              |

## Оптимизация activerecord-import

Инструменты показывают что большую часть времени скрипт производит импорт данных
с помощью библиотеки activerecord-import.
К сожалению возможности настройки параметров иморта доволько ограничены.
Большая часть времени уходит на валидации, но, как было сказано ранее, от них я
отказываться не плананирую.
Можно было бы на время импорта отключить часть валидаций в тех случаях когда мы
заранее уверены в корректности данных, но этим мне уже заниматься довольно
лениво, к тому же это действие вряд ли бы дало больше 5 процентов уменшения
времени работы скрипта (валидации занимаю 17% от полного времени работы).

## Создание AR-объектов

На создание объектов уходит 20% времени. Существенного улучшения результатов
можно добиться только если произвести полный отказ от ActiveRecord,
а следовательно и от встроенных валидаций.
По примерной оценке такая работа могда бы уменьшить время работы импорта
на 60-70%, но данная работа сделала бы скрипт менее поддерживаемым из-за
расхождения валидаций в скрипте импорта и реальных моделей. Вынесение же
валидаций в отдельный модуль не выглядит целесообразным.

## Возвращаемся к импорту автобусов

На создание автобусов уходит 26% времени. На текущий момент автобусы создаются
сразу же по одной записи. Можно было бы произвести массовый импорт автобусов
по аналогии с другими моделями, но это может создать проблемы в будущем.
Причина в том, что связь с автобусами явно указывается в модели Trip.
А следовательно записи модели Bus должны быть добавлены до записей модели Trip.
Как известно, на 1000 записей Trip приходится около 1 записи модели Bus,
следовательно для того чтобы массово сохранить хотя бы 10 записей модели Bus
на нужно было бы накопить 10000 записей модели Trip.

От данного ограничения можно было бы избавиться если вручную выставить
идентификаторы модели Bus до фактического сохранения, но и тут есть проблема,
так как если в будущем кто-то настроит внешние ключи на колонки в БД,
то скрипт внезапно перестанет работать и придётся настраивать отключение
проверки внешних ключей.

Таким образом, данную оптимизацию так же считаю нецелесообразной.

## Результаты

* large.json: 12 секунд
* 1M.json.gz: 2,5 минуты
* 10M.json.gz: предположительно 25-30 минут.
