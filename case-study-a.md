# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с 1кк данных.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала с файлом до 1к записей, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время загрузки medium файла (10к записей, 77 секунд в первой итерации)

## Гарантия корректности работы оптимизированной программы
Программа не поставлялась с тестом, поэтому перед выполнением оптимизации я добавил его самостоятельно: загрузка example файла с дальнейшим сравнением загруженных в бд данных с эталоном. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Вот как я построил `feedback_loop`: профилирование - изменение кода - тестирование – бенчмаркинг – откат при отсутствии разницы от оптимизации/сохранение результатов

## Вникаем в детали системы, чтобы найти главные точки роста
Главной точкой роста является изменение загрузки на потоковую: посимвольное чтение файла, формирование целостного объекта trip и загрузка в систему путем использования функционала COPY в PG.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными. 
Удалось улучшить метрику системы с с 77 секунд до 1.4с для medium и уложиться в заданный бюджет.
Файл large грузится за 6.5 секунд
Файл 1м стал грузится за 56 секунд.

